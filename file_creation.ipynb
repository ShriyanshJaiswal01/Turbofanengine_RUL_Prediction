{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55659f92-b15a-4601-ad13-b34b881d6472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 'datasets/CMaps/train_FD001.txt' and saved to '.\\processed_train_FD001.csv'\n",
      "Successfully processed 'datasets/CMaps/train_FD002.txt' and saved to '.\\processed_train_FD002.csv'\n",
      "Successfully processed 'datasets/CMaps/train_FD003.txt' and saved to '.\\processed_train_FD003.csv'\n",
      "Successfully processed 'datasets/CMaps/train_FD004.txt' and saved to '.\\processed_train_FD004.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_engine_data(input_filename, output_directory=\".\"):\n",
    "   \n",
    "    try:\n",
    "        raw_data = pd.read_csv(input_filename, sep=' ', header=None)\n",
    "        raw_data = raw_data.drop([26, 27], axis='columns')\n",
    "        column_names = [\n",
    "            'ID', 'Cycle', 'OpSet1', 'OpSet2', 'OpSet3', \n",
    "            'SensorMeasure1', 'SensorMeasure2', 'SensorMeasure3', 'SensorMeasure4', \n",
    "            'SensorMeasure5', 'SensorMeasure6', 'SensorMeasure7', 'SensorMeasure8', \n",
    "            'SensorMeasure9', 'SensorMeasure10', 'SensorMeasure11', 'SensorMeasure12', \n",
    "            'SensorMeasure13', 'SensorMeasure14', 'SensorMeasure15', 'SensorMeasure16', \n",
    "            'SensorMeasure17', 'SensorMeasure18', 'SensorMeasure19', 'SensorMeasure20', \n",
    "            'SensorMeasure21'\n",
    "        ]\n",
    "        raw_data.columns = column_names\n",
    "        max_cycles = raw_data.groupby('ID')['Cycle'].max().reset_index()\n",
    "        max_cycles.columns = ['ID', 'EOL']\n",
    "\n",
    "        raw_data = pd.merge(raw_data, max_cycles, on='ID', how='left')\n",
    "        raw_data['RUL'] = raw_data['EOL'] - raw_data['Cycle']\n",
    "\n",
    "        df = raw_data.drop(columns=['EOL'])\n",
    "\n",
    "        base_name = os.path.basename(input_filename)\n",
    "        output_filename = f\"processed_{os.path.splitext(base_name)[0]}.csv\"\n",
    "        output_path = os.path.join(output_directory, output_filename)\n",
    "        \n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully processed '{input_filename}' and saved to '{output_path}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_filename}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing '{input_filename}': {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files_to_process = [\n",
    "        'datasets/CMaps/train_FD001.txt',\n",
    "        'datasets/CMaps/train_FD002.txt',\n",
    "        'datasets/CMaps/train_FD003.txt',\n",
    "        'datasets/CMaps/train_FD004.txt'\n",
    "    ]\n",
    "    for file in files_to_process:\n",
    "        process_engine_data(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdd4759c-fb1c-47b1-99fe-7609eebcc4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing 'RUL_train_FD001.csv' ---\n",
      "\n",
      "Dropping 12 columns: ['OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure5', 'SensorMeasure6', 'SensorMeasure9', 'SensorMeasure10', 'SensorMeasure14', 'SensorMeasure16', 'SensorMeasure18', 'SensorMeasure19']\n",
      "Successfully created 'Processed_train_001.csv' with 15 columns.\n",
      "\n",
      "--- Processing 'RUL_train_FD002.csv' ---\n",
      "\n",
      "Dropping 12 columns: ['OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure5', 'SensorMeasure6', 'SensorMeasure9', 'SensorMeasure10', 'SensorMeasure14', 'SensorMeasure16', 'SensorMeasure18', 'SensorMeasure19']\n",
      "Successfully created 'Processed_train_002.csv' with 15 columns.\n",
      "\n",
      "--- Processing 'RUL_train_FD003.csv' ---\n",
      "\n",
      "Dropping 12 columns: ['OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure5', 'SensorMeasure6', 'SensorMeasure9', 'SensorMeasure10', 'SensorMeasure14', 'SensorMeasure16', 'SensorMeasure18', 'SensorMeasure19']\n",
      "Successfully created 'Processed_train_003.csv' with 15 columns.\n",
      "\n",
      "--- Processing 'RUL_train_FD004.csv' ---\n",
      "\n",
      "Dropping 12 columns: ['OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure5', 'SensorMeasure6', 'SensorMeasure9', 'SensorMeasure10', 'SensorMeasure14', 'SensorMeasure16', 'SensorMeasure18', 'SensorMeasure19']\n",
      "Successfully created 'Processed_train_004.csv' with 15 columns.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def remove_specified_features(input_filename):\n",
    "  \n",
    "    try:\n",
    "        \n",
    "        print(f\"--- Processing '{input_filename}' ---\")\n",
    "        df = pd.read_csv(input_filename)\n",
    "\n",
    "        columns_to_drop = [\n",
    "            'OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure5',\n",
    "            'SensorMeasure6', 'SensorMeasure9', 'SensorMeasure10', 'SensorMeasure14',\n",
    "            'SensorMeasure16', 'SensorMeasure18', 'SensorMeasure19'\n",
    "        ]\n",
    "        \n",
    "  \n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "\n",
    "        if existing_columns_to_drop:\n",
    "            print(f\"\\nDropping {len(existing_columns_to_drop)} columns: {existing_columns_to_drop}\")\n",
    "            df_filtered = df.drop(columns=existing_columns_to_drop)\n",
    "        else:\n",
    "            print(\"\\nNone of the specified columns to drop were found in the file.\")\n",
    "            df_filtered = df\n",
    "\n",
    "        match = re.search(r'FD(\\d+)', input_filename)\n",
    "        if match:\n",
    "            file_number = match.group(1)\n",
    "            output_filename = f\"Processed_train_{file_number}.csv\"\n",
    "        else:\n",
    "            base_name = os.path.basename(input_filename)\n",
    "            output_filename = f\"Processed_{os.path.splitext(base_name)[0]}.csv\"\n",
    "\n",
    "        df_filtered.to_csv(output_filename, index=False)\n",
    "        print(f\"Successfully created '{output_filename}' with {df_filtered.shape[1]} columns.\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_filename}' was not found.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing '{input_filename}': {e}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    files_to_process = [\n",
    "        'RUL_train_FD001.csv',\n",
    "        'RUL_train_FD002.csv',\n",
    "        'RUL_train_FD003.csv',\n",
    "        'RUL_train_FD004.csv'\n",
    "    ]\n",
    "\n",
    "    for file in files_to_process:\n",
    "        remove_specified_features(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ffb5f54-e943-4fac-8717-3baaf375e122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing test file set FD001 ---\n",
      "Successfully created 'Processed_Test_001.csv' with 14 columns.\n",
      "\n",
      "--- Processing test file set FD002 ---\n",
      "Successfully created 'Processed_Test_002.csv' with 14 columns.\n",
      "\n",
      "--- Processing test file set FD003 ---\n",
      "Successfully created 'Processed_Test_003.csv' with 14 columns.\n",
      "\n",
      "--- Processing test file set FD004 ---\n",
      "Successfully created 'Processed_Test_004.csv' with 14 columns.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_test_data(file_number):\n",
    "   \n",
    "    try:\n",
    "        print(f\"--- Processing test file set FD00{file_number} ---\")\n",
    "        base_path = os.path.join('datasets', 'CMaps')\n",
    "        test_filename = os.path.join(base_path, f'test_FD00{file_number}.txt')\n",
    "        \n",
    "        raw_data = pd.read_csv(test_filename, sep=' ', header=None)\n",
    "        raw_data = raw_data.drop([26, 27], axis='columns')\n",
    "        \n",
    "        column_names = [\n",
    "            'ID', 'Cycle', 'OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', \n",
    "            'SensorMeasure2', 'SensorMeasure3', 'SensorMeasure4', 'SensorMeasure5', \n",
    "            'SensorMeasure6', 'SensorMeasure7', 'SensorMeasure8', 'SensorMeasure9', \n",
    "            'SensorMeasure10', 'SensorMeasure11', 'SensorMeasure12', 'SensorMeasure13', \n",
    "            'SensorMeasure14', 'SensorMeasure15', 'SensorMeasure16', 'SensorMeasure17', \n",
    "            'SensorMeasure18', 'SensorMeasure19', 'SensorMeasure20', 'SensorMeasure21'\n",
    "        ]\n",
    "        raw_data.columns = column_names\n",
    "        rul_filename = os.path.join(base_path, f'RUL_FD00{file_number}.txt')\n",
    "        cycle_ran_after = pd.read_csv(rul_filename, sep=' ', header=None)\n",
    "        cycle_ran_after = cycle_ran_after.drop([1], axis='columns')\n",
    "        cycle_ran_after.columns = ['RUL_after_last_cycle']\n",
    "        last_cycle = raw_data.groupby('ID')['Cycle'].max().reset_index()\n",
    "        last_cycle.columns = ['ID', 'last_cycle']\n",
    "        \n",
    "        last_cycle['EOL'] = last_cycle['last_cycle'] + cycle_ran_after['RUL_after_last_cycle']\n",
    "        raw_data = pd.merge(raw_data, last_cycle[['ID', 'EOL']], on='ID', how='left')\n",
    "        raw_data['RUL'] = raw_data['EOL'] - raw_data['Cycle']\n",
    "\n",
    "        columns_to_drop = [\n",
    "            'OpSet1', 'OpSet2', 'OpSet3', 'SensorMeasure1', 'SensorMeasure5',\n",
    "            'SensorMeasure6', 'SensorMeasure9', 'SensorMeasure10', 'SensorMeasure14',\n",
    "            'SensorMeasure16', 'SensorMeasure18', 'SensorMeasure19'\n",
    "        ]\n",
    "        \n",
    "        final_df = raw_data.drop(columns=['ID', 'EOL'])\n",
    "        final_df = final_df.drop(columns=columns_to_drop)\n",
    "        output_filename = f\"Processed_Test_00{file_number}.csv\"\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Successfully created '{output_filename}' with {final_df.shape[1]} columns.\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Make sure '{test_filename}' and '{rul_filename}' exist.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing file set {file_number}: {e}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(1, 5):\n",
    "        process_test_data(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51ccad-8630-410e-ad69-c90cd5f61237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
